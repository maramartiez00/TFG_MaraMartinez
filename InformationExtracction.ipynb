{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daace6cc",
   "metadata": {},
   "source": [
    "# ALGORITMOS DE EXTRACCIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab602ae",
   "metadata": {},
   "source": [
    "En este script se encuentran los cuatro algoritmos construidos para extraer información de forma automática"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16277b8",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef693706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy \n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ecaa00",
   "metadata": {},
   "source": [
    "## 1. Background preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2f29ca",
   "metadata": {},
   "source": [
    "### 1.1. Load BD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9975f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = ''\n",
    "\n",
    "with open(data_path, encoding=\"utf8\") as fp:\n",
    "    db = json.loads(fp.read())\n",
    "    \n",
    "data = pd.DataFrame(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f42881",
   "metadata": {},
   "source": [
    "### 1.2. Insert Data_type column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f08b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_id = ['3', '5', '17', '30', '38', '39', '68', '75', '80', '84']\n",
    "not_structured_id = ['1', '2', '16', '32', '33', '40', '41', '44', '45', '52', '55', '56', '57', '58', '59', '60', '61', '62', '74', '78', '79', '81', '82', '90', '94', '100', '106']\n",
    "semi_structured_id = ['4', '6', '9', '10', '13', '15', '21', '27', '31', '35', '36', '37', '42', '63', '66', '76', '77', '83', '87', '88', '91', '92', '103']\n",
    "\n",
    "df = data.copy()\n",
    "df['Data_type'] = np.nan\n",
    "\n",
    "for idx in range(len(df)):\n",
    "    if df['Plat Id'][idx] in structured_id:\n",
    "        df['Data_type'][idx] = 'structured'\n",
    "    if df['Plat Id'][idx] in not_structured_id:\n",
    "        df['Data_type'][idx] = 'not-structured'\n",
    "    if df['Plat Id'][idx] in semi_structured_id:\n",
    "        df['Data_type'][idx] = 'semi-structured'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956a1e1c",
   "metadata": {},
   "source": [
    "### 1.3. Select english data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada0f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_data = df[df['Language'] == 'English']\n",
    "english_data = english_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a0076d",
   "metadata": {},
   "source": [
    "### 1.4. Select one random sample and clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27426374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_sample(data):\n",
    "    random_sample = data.sample(1)\n",
    "    df = pd.DataFrame()\n",
    "    #title\n",
    "    df['Title'] = random_sample['TITLE']\n",
    "    #data type\n",
    "    df['Data_type'] = random_sample['Data_type']\n",
    "    #Plat country\n",
    "    df['Country'] = random_sample['Plat country']\n",
    "    #description\n",
    "    text_list = random_sample['DESCRIPTION']\n",
    "    text_list = text_list.tolist()\n",
    "    text_list = text_list[0]\n",
    "    text = ''\n",
    "    for sentence in text_list:\n",
    "        text = text + sentence\n",
    "    print(text)\n",
    "    df['Description'] = text\n",
    "    \n",
    "    # = df.reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe3ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    \n",
    "    # removing new line characters\n",
    "    text = re.sub('\\n ','',str(text))\n",
    "    text = re.sub('\\n',' ',str(text))\n",
    "    text = text.replace('\\\\n',' ')\n",
    "    # removing hyphens\n",
    "    text = re.sub(\"-\",' ',str(text))\n",
    "    text = re.sub(\"— \",'',str(text))\n",
    "    # removing quotation marks\n",
    "    text = re.sub('\\\"','',str(text))\n",
    "    # removing salutations\n",
    "    text = re.sub(\"Mr\\.\",'Mr',str(text))\n",
    "    text = re.sub(\"Mrs\\.\",'Mrs',str(text))\n",
    "    # removing any reference to outside text\n",
    "    #text = re.sub(\"<\\[\\(\\[].*?[\\)\\]]>\", \"\", str(text))\n",
    "    text = re.sub(\"</a>\", \"\", str(text))\n",
    "    text = re.sub(\"<a\", \"\", str(text))    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc7248a",
   "metadata": {},
   "source": [
    "## 2. ALGORITHMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0dd582",
   "metadata": {},
   "source": [
    "### 2.1. Load Corpus Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274b8431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a1c630",
   "metadata": {},
   "source": [
    "### 2.2. OBJECTIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab51e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_objective(sample, doc):\n",
    "    \n",
    "    objective = None\n",
    "    #create list of string sentence\n",
    "    sents_list = []\n",
    "    for sent in doc.sents:\n",
    "        sents_list.append(sent.text)\n",
    "        \n",
    "    #lemmatizamos las no stop words\n",
    "    #es una lista de strings \n",
    "    words_list = []\n",
    "    for token in doc:\n",
    "        if token.is_stop == False and token.is_alpha==True:\n",
    "            words_list.append(token.lemma_)\n",
    "                \n",
    "        \n",
    "    #palabras clave\n",
    "    words = ['goal', 'purpose', 'objective', 'intention', 'ambition', 'promote', 'dedicate']\n",
    "    synonyms = []\n",
    "    for w in words:\n",
    "        for synset in wordnet.synsets(w):\n",
    "            for i in synset.lemmas():\n",
    "                if i.name() not in synonyms and i.name()!='use':\n",
    "                    synonyms.append(i.name())\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Miramos si alguna palabra de la lista words esta en el texto\n",
    "    for token in doc:\n",
    "        if token.text in words:\n",
    "            for sentence in sents_list:\n",
    "                    if token.text in sentence.split(' '):\n",
    "                        objective = sentence\n",
    "                        return objective\n",
    "    \n",
    "    \n",
    "    #Mirar si alguna palabra de words_list esta en synonyms, si lo esta entonces devolver esa frase donde esta esa word\n",
    "    for token in doc:\n",
    "        if token.is_stop == False and token.is_alpha==True:\n",
    "            if (token.text in synonyms) or (token.lemma_ in synonyms):\n",
    "                for sentence in sents_list:\n",
    "                    if token.text in sentence.split(' '):\n",
    "                        objective = sentence\n",
    "                        return objective\n",
    "    \n",
    "    #Mirar la similitud entre las palabras de synonyms y todas las palabras de words_list y si supera un umbral\n",
    "    #devolver la frase donde esta esa word\n",
    "    threshold = 0.7\n",
    "    tokens_list = []\n",
    "    for token in doc:\n",
    "        word1 = nlp(token.text)\n",
    "        for synonym in synonyms:\n",
    "            word2 = nlp(synonym)\n",
    "            if word1.similarity(word2) >= threshold:\n",
    "                tokens_list.append(token.text)\n",
    "                \n",
    "                \n",
    "    for sent in sents_list:\n",
    "        for tok in tokens_list:\n",
    "            if tok in sent:\n",
    "                objective = sent\n",
    "                return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af30b3",
   "metadata": {},
   "source": [
    "### 2.3. GELOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b5263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_location(sample, doc):\n",
    "    locations = []\n",
    "    location = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['LOC']:\n",
    "            locations.append(ent.text)\n",
    "        if ent.label_ in ['GPE']:\n",
    "            locations.append(ent.text)\n",
    "\n",
    "        #intento de añadir gentilicios\n",
    "        '''if ent.label_ in ['NORP']:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            word = ent.text\n",
    "            word = word.lower()\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            locations.append(word)'''\n",
    "\n",
    "    if len(locations) == 0:\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        locations = Counter(locations).most_common(1)\n",
    "        for t in locations:\n",
    "            location.append(t[0])\n",
    "    \n",
    "    return location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e47000",
   "metadata": {},
   "source": [
    "### 2.4. ORGANIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58da1ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_organization(sample, doc):\n",
    "    organizations = []\n",
    "    organization = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['ORG']:\n",
    "            #si la entidad es identificada como organización pero es igual que el titulo, la descartamos \n",
    "            #porque no es una organizacion sino el nombre del proyecto\n",
    "            if sample['Title'].tolist()[0] not in ent.text:\n",
    "                if ent not in organizations:\n",
    "                    organizations.append(ent)\n",
    "    if len(organizations) == 0:\n",
    "        return [None]\n",
    "    else:\n",
    "        organizations = Counter(organizations).most_common(1)\n",
    "        for t in organizations:\n",
    "            organization.append(t[0])\n",
    "\n",
    "    return organization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8c543b",
   "metadata": {},
   "source": [
    "### 2.5. PARTICIPANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7ffb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_participants(sample, doc):\n",
    "    \n",
    "    sents_list = []\n",
    "    for sent in doc.sents:\n",
    "        sents_list.append(sent.text)\n",
    "    \n",
    "    categories = ['students', 'University students', 'kids', 'adults', 'community', '18 years', 'group of', 'undergraduate students']\n",
    "    \n",
    "    #Gentilicios\n",
    "    loc = extract_location(rs, doc)\n",
    "    \n",
    "    participants = []\n",
    "    \n",
    "    for sent in sents_list:\n",
    "        for category in categories:\n",
    "            if category in sent:\n",
    "                participants.append(category)\n",
    "    \n",
    "    '''for token in doc:\n",
    "        if token.text in categories:\n",
    "            participants.append(token.text)'''\n",
    "        \n",
    "    if len(participants) == 0: #Si no especifica devolvemos Anyone\n",
    "        return \"Anyone\"\n",
    "    \n",
    "    elif categories[6] in participants:\n",
    "        for sent in doc.sents:\n",
    "            s = sent.text\n",
    "            if categories[6] in s:\n",
    "                words_list = s.split(' ')\n",
    "                for i in range(len(words_list)):\n",
    "                    if words_list[i] == 'group' and words_list[i+1] == 'of':\n",
    "                        return(words_list[i] + ' ' + words_list[i+1] + ' '+ words_list[i+2])\n",
    "    \n",
    "    elif categories[4] in participants:\n",
    "        if loc == None:\n",
    "                participant = 'Area community'\n",
    "        else:\n",
    "            participant = \"Area community (\" + loc[0] + ')'\n",
    "        return participant\n",
    "    \n",
    "    elif (categories[3] in participants) or (categories[5] in participants): #si existe \"adults\" o \"18 years\" se categoriza como adults\n",
    "        return 'Adults'      \n",
    "    \n",
    "    else:\n",
    "        return participants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42010908",
   "metadata": {},
   "source": [
    "### 2.6. DATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a954a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dates(sample, doc):\n",
    "    dates = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['DATE']:\n",
    "            dates.append(ent)\n",
    "        if ent.label_ in ['TIME']:\n",
    "            dates.append(ent)\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c156bd",
   "metadata": {},
   "source": [
    "## 3. PROCESSING TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfdfe05",
   "metadata": {},
   "source": [
    "### 3.1. Select one random text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28d840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = select_random_sample(english_data)\n",
    "text = rs['Description'].to_list()\n",
    "text = clean(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed59885d",
   "metadata": {},
   "source": [
    "### 3.2. Create nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5debddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "#para visualizar entidades nombradas del texto\n",
    "#displacy.render(doc, style='ent')\n",
    "\n",
    "#para saber que significan las entidades:\n",
    "#spacy.explain(\"FAC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf86d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy.explain(\"GPE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b75e47",
   "metadata": {},
   "source": [
    "### 3.3. Apply algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a06631",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LOCATION:', extract_location(rs, doc))\n",
    "print('ORGANIZATION/S:', extract_organization(rs, doc))\n",
    "print('DATE/S or DURATION:', extract_dates(rs, doc))\n",
    "print('PARTICIPANTS: ', extract_participants(rs, doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbfb2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('OBJECTIVE: ', extract_objective(rs, doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0cf63a",
   "metadata": {},
   "source": [
    "## 4. CSV CREATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef5dad",
   "metadata": {},
   "source": [
    "### 4.1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994ede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    df = pd.DataFrame()\n",
    "    #title\n",
    "    df['Title'] = data['TITLE']\n",
    "    #data type\n",
    "    df['Data_type'] = data['Data_type']\n",
    "    #Plat country\n",
    "    df['Country'] = data['Plat country']\n",
    "    #Description\n",
    "    df['Description'] = data['DESCRIPTION']\n",
    "    \n",
    "    #Add algorithms columns\n",
    "    df['Objective'] = ''\n",
    "    df['Geolocalization'] = ''\n",
    "    df['Organization'] = ''\n",
    "    df['Participants'] = ''\n",
    "    \n",
    "    df = df.reset_index()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1239b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temp_register(data, index):\n",
    "    df = pd.DataFrame()\n",
    "    df = data.iloc[[index]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a95156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_description(data, index):\n",
    "    text_list = [data['Description'][index]]\n",
    "    #text_list = text_list.tolist()\n",
    "    text_list = text_list[0]\n",
    "    text = ''\n",
    "    for sentence in text_list:\n",
    "        if type(sentence) == str:\n",
    "            text = text + sentence\n",
    "        elif type(sentence) == int:\n",
    "            text = text + str(sentence)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f9d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = clean_data(english_data)\n",
    "text = extract_description(d, 0)\n",
    "text = clean(text)\n",
    "#------------\n",
    "x = create_temp_register(d, 0)\n",
    "doc = nlp(text)\n",
    "loc = extract_participants(x, doc)\n",
    "print(type(loc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544b6cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(d['Description'][2468]) == float:\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13234c83",
   "metadata": {},
   "source": [
    "### 4.2. Information extraction (all english data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c278ae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_creation(data):\n",
    "    #Select only the columns need it\n",
    "    imp_data = clean_data(data)\n",
    "    for i in data.index:\n",
    "        print(i)\n",
    "        if type(imp_data['Description'][i]) == float:\n",
    "            imp_data['Geolocalization'][i] = None\n",
    "            imp_data['Objective'][i] = None\n",
    "            imp_data['Organization'][i] = None\n",
    "            imp_data['Participants'][i] = None\n",
    "            \n",
    "        else:\n",
    "            text = extract_description(imp_data, i)\n",
    "            text = clean(text)\n",
    "            doc = nlp(text)\n",
    "            temp_data = create_temp_register(imp_data, i)\n",
    "            imp_data['Geolocalization'][i] = extract_location(temp_data, doc)\n",
    "            #imp_data['Objective'][i] = extract_objective(temp_data, doc)\n",
    "            imp_data['Organization'][i] = extract_organization(temp_data, doc)\n",
    "            imp_data['Participants'][i] = extract_participants(temp_data, doc)\n",
    "    return imp_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb74d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_creation(data):\n",
    "    #Select only the columns need it\n",
    "    imp_data = clean_data(data)\n",
    "    objective = [None]*data.index\n",
    "    for i in range(200):\n",
    "        print(i)\n",
    "        if type(imp_data['Description'][i]) == float:\n",
    "            pass\n",
    "        else:\n",
    "            text = extract_description(imp_data, i)\n",
    "            text = clean(text)\n",
    "            doc = nlp(text)\n",
    "            temp_data = create_temp_register(imp_data, i)\n",
    "            objective[i] = extract_objective(temp_data, doc)\n",
    "    \n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e296086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = db_creation(english_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d216b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_list = objective_creation(english_data)\n",
    "obj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e143b1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "none_list = [None]*(len(english_data)-len(obj_list))\n",
    "obj_list = obj_list + none_list\n",
    "obj_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea52fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie['Description'] = english_data['DESCRIPTION']\n",
    "ie['Objective'] = obj_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3706775e",
   "metadata": {},
   "source": [
    "### 4.3. CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9f1ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie.to_csv('InformationExtraction_reduced.csv', index = False, sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849eae1c",
   "metadata": {},
   "source": [
    "## 5. MOST COMMON WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2caaee",
   "metadata": {},
   "source": [
    "### 5.1. List to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed3cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "for i in range(200):\n",
    "    if obj_list[i] != None:\n",
    "        text = text + obj_list[i]\n",
    "        text = text + ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19387e8",
   "metadata": {},
   "source": [
    "### 5.2. Clean text (remove stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d35e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f186f03d",
   "metadata": {},
   "source": [
    "Add the most tipic words in objective seentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc5fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['goal', 'purpose', 'objective', 'intention', 'ambition', 'promote', 'dedicate', 'aims']\n",
    "synonyms = []\n",
    "for w in words:\n",
    "    for synset in wordnet.synsets(w):\n",
    "        for i in synset.lemmas():\n",
    "            if i.name() not in synonyms and i.name()!='use':\n",
    "                synonyms.append(i.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f23f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords_gensim = STOPWORDS.union(set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4de985",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = ''\n",
    "for word in text.split(' '):\n",
    "    if word not in all_stopwords_gensim:\n",
    "        clean_text = clean_text + word + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a602c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc9028ca",
   "metadata": {},
   "source": [
    "### 5.2. WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e54c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090a1acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_words=150, background_color=\"white\").generate(clean_text)\n",
    "plt.figure(figsize = (15,15))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ceeb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.to_file(\"WordCloud.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1be170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
